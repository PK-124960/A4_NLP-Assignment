{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d3b4a4",
   "metadata": {},
   "source": [
    "# ðŸ“˜ NLP Assignment: Custom BERT Model for Natural Language Inference (NLI)\n",
    "\n",
    "âœ… **Task 1:** Training a custom BERT model from scratch using the BookCorpus dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4eacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba1ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39231fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (BookCorpus subset)\n",
    "dataset = load_dataset('bookcorpus', split='train[:1%]')\n",
    "sentences = dataset['text'][:50000]  # Using only 100K sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517f2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27205c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization & Masking\n",
    "def tokenize_and_mask(sentences, tokenizer, mask_prob=0.20):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    labels = inputs.input_ids.clone()\n",
    "    rand = torch.rand(labels.shape)\n",
    "    mask_arr = (rand < mask_prob) & (labels != tokenizer.pad_token_id)\n",
    "    \n",
    "    inputs.input_ids[mask_arr] = tokenizer.mask_token_id\n",
    "    labels[~mask_arr] = -100  # Ignore loss for unmasked tokens\n",
    "    \n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaaa87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom BERT Model\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=512, num_heads=8, num_layers=6):\n",
    "        super(CustomBERT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads), num_layers\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.encoder_layers(x)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33bf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer and model\n",
    "tokenized_data, labels = tokenize_and_mask(sentences, tokenizer)\n",
    "tokenized_data = {key: val.to(device) for key, val in tokenized_data.items()}\n",
    "labels = labels.to(device)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = CustomBERT(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d7d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9156879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4  # Adjust based on memory\n",
    "\n",
    "def train_model(model, data, labels, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        for i in range(0, len(data[\"input_ids\"]), accumulation_steps):\n",
    "            batch_input = data[\"input_ids\"][i : i + accumulation_steps]\n",
    "            batch_labels = labels[i : i + accumulation_steps]\n",
    "\n",
    "            outputs = model(batch_input)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), batch_labels.view(-1))\n",
    "            loss = loss / accumulation_steps  # Normalize loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f604da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.6512725353240967\n",
      "Epoch 2, Loss: 2.635631561279297\n",
      "Epoch 3, Loss: 2.6408047676086426\n",
      "Epoch 4, Loss: 2.6295759677886963\n",
      "Epoch 5, Loss: 2.658684253692627\n",
      "Epoch 6, Loss: 2.648127794265747\n",
      "Epoch 7, Loss: 2.6480934619903564\n",
      "Epoch 8, Loss: 2.645524024963379\n",
      "Epoch 9, Loss: 2.6441519260406494\n",
      "Epoch 10, Loss: 2.643312692642212\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, tokenized_data, labels, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786856ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved for Task 2.\n"
     ]
    }
   ],
   "source": [
    "# Save trained model weights\n",
    "torch.save(model.state_dict(), \"trained_bert_model.pth\")\n",
    "print(\"Model training complete and saved for Task 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b967456",
   "metadata": {},
   "source": [
    "âœ… **Task 2:** Fine-tuning the model for Sentence-BERT (SBERT) on the SNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e80b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load trained model configuration\n",
    "checkpoint = torch.load(\"trained_bert_model.pth\", map_location=\"cpu\")\n",
    "\n",
    "# Get hidden size dynamically (512 or 768)\n",
    "hidden_dim = checkpoint[\"embedding.weight\"].shape[1]\n",
    "\n",
    "# Ensure num_heads is a valid divisor of hidden_dim\n",
    "num_heads = max(1, hidden_dim // 64)  # Ensure divisibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f259def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=hidden_dim, num_heads=num_heads, num_layers=12):\n",
    "        super(CustomBERT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads), num_layers\n",
    "        )\n",
    "        # Keep the original output layer dimensions from checkpoint\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        # Add a new projection layer for the classifier\n",
    "        self.projection = nn.Linear(vocab_size, hidden_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.encoder_layers(x)\n",
    "        x = x.mean(dim=1)  # Pooling to get a single vector per input\n",
    "        x = self.output_layer(x)  # First get the vocab_size dimensional output\n",
    "        x = self.projection(x)    # Project back to hidden_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dcaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained Custom BERT model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = 30522  # Standard BERT vocab size\n",
    "bert_model = CustomBERT(vocab_size, hidden_dim=hidden_dim, num_heads=num_heads).to(device)\n",
    "\n",
    "# Load model weights (ignore extra/missing keys)\n",
    "bert_model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"snli\")  # Use SNLI dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a763cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_pairs(example):\n",
    "    tokens1 = tokenizer(example[\"premise\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    tokens2 = tokenizer(example[\"hypothesis\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    # Convert label to integer (Handle SNLI 'test' set, which has no labels)\n",
    "    label = example[\"label\"]\n",
    "    if isinstance(label, str) and not label.isdigit():\n",
    "        label = -1  # Assign -1 for test set (since it has no labels)\n",
    "    else:\n",
    "        label = int(label)  # Convert to integer\n",
    "    \n",
    "    return {\n",
    "        \"input_ids1\": tokens1[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask1\": tokens1[\"attention_mask\"].squeeze(0),\n",
    "        \"input_ids2\": tokens2[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask2\": tokens2[\"attention_mask\"].squeeze(0),\n",
    "        \"label\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008ad85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data sample: {'label': 1, 'input_ids1': [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask1': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids2': [101, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask2': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Total samples in dataset: 550152\n"
     ]
    }
   ],
   "source": [
    "# Apply mapping function and ensure dataset is correctly formatted\n",
    "dataset = dataset.map(tokenize_pairs, remove_columns=[col for col in [\"premise\", \"hypothesis\"] if col in dataset[\"train\"].column_names])  # Remove only existing columns\n",
    "\n",
    "# Convert dataset to list correctly\n",
    "dataset_list = [dict(d) for d in dataset[\"train\"]]\n",
    "\n",
    "# Debugging output: Check first sample\n",
    "print(\"First data sample:\", dataset_list[0])\n",
    "print(f\"Total samples in dataset: {len(dataset_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3d632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have valid tensors before stacking\n",
    "input_ids1_list = [torch.tensor(d[\"input_ids1\"], dtype=torch.long) for d in dataset_list if \"input_ids1\" in d]\n",
    "attention_mask1_list = [torch.tensor(d[\"attention_mask1\"], dtype=torch.long) for d in dataset_list if \"attention_mask1\" in d]\n",
    "input_ids2_list = [torch.tensor(d[\"input_ids2\"], dtype=torch.long) for d in dataset_list if \"input_ids2\" in d]\n",
    "attention_mask2_list = [torch.tensor(d[\"attention_mask2\"], dtype=torch.long) for d in dataset_list if \"attention_mask2\" in d]\n",
    "labels_list = [torch.tensor(d[\"label\"], dtype=torch.long) for d in dataset_list if isinstance(d[\"label\"], int)]\n",
    "\n",
    "# Ensure lists are not empty\n",
    "if not input_ids1_list or not input_ids2_list:\n",
    "    raise RuntimeError(\"Tokenized data is empty. Verify tokenize_pairs() function.\")\n",
    "\n",
    "# Stack tensors\n",
    "input_ids1 = torch.stack(input_ids1_list)\n",
    "attention_mask1 = torch.stack(attention_mask1_list)\n",
    "input_ids2 = torch.stack(input_ids2_list)\n",
    "attention_mask2 = torch.stack(attention_mask2_list)\n",
    "labels = torch.stack(labels_list)\n",
    "\n",
    "# Ensure labels are within the valid range\n",
    "valid_labels = (labels >= 0) & (labels < 3)\n",
    "input_ids1 = input_ids1[valid_labels]\n",
    "attention_mask1 = attention_mask1[valid_labels]\n",
    "input_ids2 = input_ids2[valid_labels]\n",
    "attention_mask2 = attention_mask2[valid_labels]\n",
    "labels = labels[valid_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a450a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8682435154914856\n",
      "Epoch 2 Loss: 0.7968205213546753\n",
      "Epoch 3 Loss: 0.6686006188392639\n",
      "Epoch 4 Loss: 1.0666700601577759\n",
      "Epoch 5 Loss: 0.92558354139328\n",
      "Epoch 6 Loss: 0.4347539246082306\n",
      "Epoch 7 Loss: 0.9607365727424622\n",
      "Epoch 8 Loss: 1.2120252847671509\n",
      "Epoch 9 Loss: 0.6323928833007812\n",
      "Epoch 10 Loss: 0.17150042951107025\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids1, attention_mask1, input_ids2, attention_mask2, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define Classifier Head\n",
    "classifier_head = nn.Linear(hidden_dim * 3, 3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer = optim.Adam(bert_model.parameters(), lr=1e-5)\n",
    "optimizer_classifier = optim.Adam(classifier_head.parameters(), lr=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    bert_model.train()\n",
    "    classifier_head.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # Get embeddings from BERT\n",
    "        output1 = bert_model(input_ids1)  # Shape: (batch_size, hidden_dim)\n",
    "        output2 = bert_model(input_ids2)  # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([\n",
    "            output1,\n",
    "            output2,\n",
    "            torch.abs(output1 - output2)\n",
    "        ], dim=1)  # Shape: (batch_size, hidden_dim * 3)\n",
    "        \n",
    "        outputs = classifier_head(combined)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a13a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and classifier for Task 3\n",
    "torch.save(bert_model.state_dict(), \"sbert_model.pth\")\n",
    "torch.save(classifier_head.state_dict(), \"classifier_head.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a9cd5",
   "metadata": {},
   "source": [
    "âœ… **Task 3:** Evaluating the trained model and computing performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f767b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ae7c560a4b4aab8afbf361d65a1afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7544\n",
      "Precision: 0.7561, Recall: 0.7544, F1-score: 0.7531\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load trained SBERT model and classifier head\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_heads, num_layers=12):\n",
    "        super(CustomBERT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads), num_layers\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.projection = nn.Linear(vocab_size, hidden_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.encoder_layers(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# Load trained model and classifier\n",
    "checkpoint = torch.load(\"sbert_model.pth\", map_location=device)\n",
    "hidden_dim = checkpoint[\"embedding.weight\"].shape[1]\n",
    "num_heads = max(1, hidden_dim // 64)\n",
    "\n",
    "bert_model = CustomBERT(30522, hidden_dim, num_heads).to(device)\n",
    "bert_model.load_state_dict(checkpoint)\n",
    "bert_model.eval()\n",
    "\n",
    "classifier_head = nn.Linear(hidden_dim * 3, 3).to(device)\n",
    "classifier_head.load_state_dict(torch.load(\"classifier_head.pth\", map_location=device))\n",
    "classifier_head.eval()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"snli\", split=\"test\")\n",
    "\n",
    "def tokenize_pairs(example):\n",
    "    tokens1 = tokenizer(example[\"premise\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    tokens2 = tokenizer(example[\"hypothesis\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids1\": tokens1[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask1\": tokens1[\"attention_mask\"].squeeze(0),\n",
    "        \"input_ids2\": tokens2[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask2\": tokens2[\"attention_mask\"].squeeze(0),\n",
    "        \"label\": example[\"label\"] if isinstance(example[\"label\"], int) else -1\n",
    "    }\n",
    "\n",
    "# Apply mapping function and filter valid labels\n",
    "dataset = dataset.map(tokenize_pairs, remove_columns=[\"premise\", \"hypothesis\"])\n",
    "dataset_list = [dict(d) for d in dataset if d[\"label\"] >= 0]\n",
    "\n",
    "input_ids1 = torch.stack([torch.tensor(d[\"input_ids1\"]) for d in dataset_list])\n",
    "attention_mask1 = torch.stack([torch.tensor(d[\"attention_mask1\"]) for d in dataset_list])\n",
    "input_ids2 = torch.stack([torch.tensor(d[\"input_ids2\"]) for d in dataset_list])\n",
    "attention_mask2 = torch.stack([torch.tensor(d[\"attention_mask2\"]) for d in dataset_list])\n",
    "labels = torch.tensor([d[\"label\"] for d in dataset_list])\n",
    "\n",
    "dataset = TensorDataset(input_ids1, attention_mask1, input_ids2, attention_mask2, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, labels = [b.to(device) for b in batch]\n",
    "        output1 = bert_model(input_ids1)\n",
    "        output2 = bert_model(input_ids2)\n",
    "        combined = torch.cat([output1, output2, torch.abs(output1 - output2)], dim=1)\n",
    "        outputs = classifier_head(combined)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
